\documentclass{article}

\usepackage[final]{neurips_2023}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % graphics
\usepackage{subcaption}     % subfigures


\title{nnUNet Segmentation on Hippocampus MRI}
\author{
  Ziyao Wang\\
  73752594\\
  \texttt{ziywang@student.ubc.ca} \\
}

\begin{document}

\maketitle

\begin{abstract}
  The nnUNet is a CNN-based semantic segmentation method that is designed to work out-of-the-box in biomedical domain. It can automatically adapt to the given dataset and configure a corresponding U-Net segmentation pipeline, and is therefore widely adopted by medical imaging scientists. In this way, however, nnUNet trades customization for generalization and may not perform good enough in a specific dataset. In the report, we use nnUNet to segment hippocampus MRI images and modify its code to expect an improvement in the training accuracy and training time. We conclude that placing activation function before batch normalization in nnUNet achieves faster training with slightly better accuracy for hippocampus MRI. Also, early stop can be introduced in nnUNet training process to reduce overfitting for a relatively small dataset.
\end{abstract}


\section{Introduction}

The nnUNet is a CNN-based semantic segmentation method that is designed to work out-of-the-box in biomedical domain. Given a new dataset, nnUNet will first analyze it and extract its properties such as size, resolution, and foreground intensity as a dataset fingerprint. Then, it creates several configurations for the dataset, namely \texttt{2D}, \texttt{3D\_fullres}, and \texttt{3D\_fullres\_cascade}. After that, it set model parameters for each configuration based on the dataset fingerprint and hard-coded rules. Finally, nnUNet goes through training processes of each configuration and compares them to find the one with best validation accuracy. This whole pipeline is carried out automatically with simple CLI commands, which is good for non-experts in the machine learning field.

In this way, however, nnUNet trades customization for generalization and may not perform good enough in a specific dataset. Also, there are some unnecessary processes in the pipeline if you know the dataset well. For instance, hippocampus MRI images are all captured in 3D, and thus training with \texttt{2D} configuration is a waste of time. MRI has a typical spatial resolution \(\Delta=1\)\texttt{mm}, and specify it in the dataset will ensure parameters to be appropriate. More tuning can be made to nnUNet to segment a specific dataset.

In the report, we use the dataset of hippocampus MRI taken from Medical Segmentation Decathlon. We take half of the training set with labels as test set. The remaining half is used for training based on 5-fold cross validation. We use nnUNet to segment the dataset and modify its code to expect an improvement in the training accuracy and training time. On one hand, we swap the order of activation function and batch normalization in the neural network and observe a faster training per epoch with quite similar accuracy. On the other hand, we notice the loss function doesn't make any progress after some epochs and find that early stop can be introduced to faster terminate the training process without loss of test accuracy and reduce overfitting for this small dataset.

\section{Related work}

There have been numerous CNN-based segmentation works on hippocampus MRI, and here we mainly focus on the models used. It's acknowledged that the U-Net purposed by \citet{ronneberger2015u} is the origin of CNN-based medical image segmentation. As shown in Figure \ref{unetarch}, U-Net consists of double 3x3 convolutions, each followed by ReLU and a 2x2 max pooling operation with stride 2 for downsampling. The original U-Net is only for segmenting 2D biomedical images. There is no normalization process in U-Net and therefore it is vulnerable to gradient explosion, namely too large gradient lengthens the training time. Batch normalization purposed by \citet{ioffe2015batch} in the same year of U-Net fixes this. By putting normalization between each convolution and ReLU, the range of layer input is constrained and the gradient obtained from ReLU is also constrained from explosion. Batch normalization in this way means much faster training time. However, there's a debate in design of network on the order of batch normalization and activation function, and which of them comes first would affect slightly training time and accuracy.

\citet{isensee2021nnu} creates nnUNet from the original 2D U-Net and the later 3D U-Net, and add batch normalization between convolution and ReLU to boost training speed. Apart from that, nnUNet is designed to be out-of-the-box for non-experts by automatically finding the best configuration (\texttt{2D}, \texttt{3D\_fullres}, \texttt{3D\_fullres\_cascade}) and appropriate parameters for a given dataset, as shown in Figure \ref{nnunetarch}. However, nnUNet forces each training to last 1000 epochs and this may be too long or unnecessary for some small dataset.

From prior work, two things we focused are the order of normalization and ReLU, and the training epochs in nnUNet.

\begin{figure}
  \centering
  \includegraphics[scale=0.18]{./figs/unetarch.png}
  \caption{U-Net architecture.}
  \label{unetarch}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.22]{./figs/nnunetarch.png}
  \caption{nnUNet architecture.}
  \label{nnunetarch}
\end{figure}

\section{nnUNet segmentation on hippocampus MRI}

\subsection{Hippocampus}

As shown in Figure \ref{hippoimg} A, a hippocampus comprises Ammon's horn (CA1 --- CA4) and dentate gyrus (2). They are like ropes bundled together to form the hippocampus --- a bundle of ropes. \citet{alves2022imaging} shows that the best modality to image hippocampus is T2-weighted Magnetic Resonance Imaging (MRI), shown in Figure \ref{hippoimg} B. MRI offers good soft tissue contrast deep in brain and is capable to penetrate bones. T2-weighted MRI makes white matter darker than grey matter and shows hippocampus boundary better than T1. Figure \ref{hippoimg} C is T1-weighted MRI in axial view and from it we can see a pair of hippocampus in human brain with head (9), body (10), and tail (11).

There is also CT-based hippocampus imaging, but its low soft tissue contrast requires tracers and more robust CNN model to segment. Ultrasound imaging is an alternative to MRI when it comes to fetal hippocampus, with better spatial resolution at a lower cost.

We obtain the hippocampus MRI data from Medical Segmentation Decathlon held in 2018, as described by \citet{simpson2019large}. The target image covers hippocampus head and body, with three types of labels distinguishing them and background. As shown in Figure \ref{hippolabel}, the head is marked as grey, the body white, and black for background. The main challenge is to segment neighboring hippocampus head and body with high precision.

\begin{figure}
  \centering
  \includegraphics[scale=0.53]{./figs/hippoimg.png}
  \caption{Coronal (A, B) and axial (C) views of hippocampus.}
  \label{hippoimg}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.12]{./figs/hippolabel.png}
  \caption{Sagittal view of hippocampus image (left) and label image (right).}
  \label{hippolabel}
\end{figure}

\subsection{Training and testing}

The dataset comprises 260 hippocampus images with 260 label images. We take half of them as test set, and for the other half, we conduct 5-fold cross validation to train our nnUNet model using nnuNet \texttt{3D} configuration with default parameters, and each fold lasts 1000 epochs. nnUNet uses double 3x3 convolutions similar with U-Net, each followed by z-score normalization and then leaky ReLU activation function. Figure \ref{nnunetwork} illustrates the structure of the neural network, where each black box is two copy of Conv3x3x3 \(\rightarrow\) zscore \(\rightarrow\) leakyReLU.

Testing is done by first predicting the labels of test set and then comparing them with ground truth labels. We use dice and IoU coefficients to calculate prediction accuracy, as shown in Figure \ref{diceiou}, taken from \citet{huynh2023understanding}. Both of them have a range of \([0,1]\), where 1 indicates perfect prediction, but IoU penalizes more on low accuracy and is able to show more slightly difference.

\begin{figure}
  \centering
  \includegraphics[scale=0.19]{./figs/nnunetwork.png}
  \caption{nnUNet structure.}
  \label{nnunetwork}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{./figs/dice.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{./figs/iou.png}
  \end{subfigure}
  \caption{Dice and IoU.}
  \label{diceiou}
\end{figure}

\section{Experiment and analysis}

\subsection{Order of z-score and leaky ReLU}

Apart from the Conv3x3x3 \(\rightarrow\) zscore \(\rightarrow\) leakyReLU configuration used in nnUNet by default, there's another option to swap normalization and activation function, namely Conv3x3x3 \(\rightarrow\) leakyReLU \(\rightarrow\) zscore. Since this swap is unavailable in nnUNet options, we modify the implementation of the network to make it happen. After modification, we carry out training and testing again, and record the progress and results.

Figure \ref{progress} shows the progress in 1000 epochs to train fold 0. During training process, both training and validation loss sharply decreases at first, and then oscillating around some value. The pseudo dice is the random approximation of dice coefficient, and can be considered as the measurement of validation accuracy on each epoch.

If we modify the code to apply leakyReLU first, shown in Figure \ref{progressb}, the pseudo dice progresses to the oscillating state faster than default configuration (notice the axis value on the right side), and average training time per epoch is also faster (7.75s compared with 8.12s). This could lead to faster training time when combined with early stop.

Table \ref{results} lists the prediction accuracy for hippocampus head and body as per dice and IoU. According to the results, we conclude that hippocampus head is easier to segment than body, and the order of normalization and activation function doesn't affect prediction accuracy of nnUNet. Thus, placing leakyReLU first can achieve shorter epoch time with similar accuracy.

\begin{figure}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{./figs/normreluprogress.png}
    \caption{Conv3x3x3 \(\rightarrow\) zscore \(\rightarrow\) leakyReLU}
    \label{progressa}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{./figs/relunormprogress.png}
    \caption{Conv3x3x3 \(\rightarrow\) leakyReLU \(\rightarrow\) zscore}
    \label{progressb}
  \end{subfigure}
  \caption{Training progress in 1000 epochs.}
  \label{progress}
\end{figure}

\begin{table}
  \caption{nnUNet results}
  \label{results}
  \centering
  \begin{tabular}{ccccc}
    \toprule
    & \multicolumn{2}{c}{Validation} & \multicolumn{2}{c}{Test} \\
    \cmidrule(r){2-5}
    & Dice & IoU & Dice & IoU \\
    \midrule
    zscore \(\rightarrow\) ReLU head &0.9107&0.8379&0.8840&0.7941\\
    \cmidrule(r){1-1}
    zscore \(\rightarrow\) ReLU body &0.8989&0.8180&0.8634&0.7616\\
    \cmidrule(r){1-1}
    ReLU \(\rightarrow\) zscore head &0.9104&0.8372&0.8843&0.7945\\
    \cmidrule(r){1-1}
    ReLU \(\rightarrow\) zscore body &0.8989&0.8181&0.8635&0.7618\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Early stop of epochs}

From Figure \ref{progress}, the validation loss slightly increases as training loss decreases in high epochs, which implies model overfitting. To overcome that, except for cross validation, we can limit the number of epochs. For this small hippocampus MRI dataset (130 training data), 200 epochs would give good enough results. For other larger dataset, minimum epochs to enter the oscillating state would increase.

\section{Discussion and future work}

In this report, we conduct nnUNet-based segmentation on hippocampus MRI, and analyze the influence on training time and prediction accuracy by swapping normalization and ReLU, and introducing early stop in training. We conclude that placing ReLU first would reduce training time per epoch with similar prediction accuracy, and early stop is good enough for small dataset. The future work will be tuning default parameters in nnUNet to achieve better accuracy, and implementation of pseudo-dice-based early stop.

\bibliographystyle{abbrvnat}
\bibliography{report}


\end{document}